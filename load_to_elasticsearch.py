"""
–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Elasticsearch —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ chunks
"""

import os
from pathlib import Path

from test_llm.package.elastic import ElasticsearchClient

from typing import List
from elasticsearch import Elasticsearch


def check_elasticsearch_connection(es_host: str = "localhost", es_port: int = 9200) -> Elasticsearch:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Elasticsearch
    """
    # es = Elasticsearch([f"http://{es_host}:{es_port}"])

    es = ElasticsearchClient()
    es = es.es
    
    if not es.ping():
        raise ConnectionError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Elasticsearch –Ω–∞ {es_host}:{es_port}")
    
    print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Elasticsearch: {es_host}:{es_port}")
    return es


def create_index(es: Elasticsearch, index_name: str) -> None:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    """
    # –£–¥–∞–ª–∏—Ç—å –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if es.indices.exists(index=index_name):
        print(f"‚ö†Ô∏è  –ò–Ω–¥–µ–∫—Å '{index_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –£–¥–∞–ª—è–µ–º...")
        es.indices.delete(index=index_name)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
    settings = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "russian": {
                        "type": "standard"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "content": {
                    "type": "text",
                    "analyzer": "russian"
                },
                "filename": {
                    "type": "keyword"
                },
                "chunk_id": {
                    "type": "integer"
                },
                "total_chunks": {
                    "type": "integer"
                }
            }
        }
    }
    
    es.indices.create(index=index_name, body=settings)
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å '{index_name}' —Å–æ–∑–¥–∞–Ω")


def split_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ chunks —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        chunk_size: –†–∞–∑–º–µ—Ä chunk –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É chunks
    
    Returns:
        –°–ø–∏—Å–æ–∫ chunks
    """
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        # –ö–æ–Ω–µ—Ü chunk
        end = min(start + chunk_size, text_length)
        
        # –í—ã—Ä–µ–∑–∞–µ–º chunk
        chunk = text[start:end].strip()
        
        if chunk:  # –ï—Å–ª–∏ chunk –Ω–µ –ø—É—Å—Ç–æ–π
            chunks.append(chunk)
        
        # –°–ª–µ–¥—É—é—â–∞—è –ø–æ–∑–∏—Ü–∏—è —Å —É—á–µ—Ç–æ–º overlap
        start = end - overlap
        
        # –ò–∑–±–µ–≥–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
        if start >= text_length - overlap:
            break
    
    return chunks


def find_documents(docs_path: str = "data/documents") -> List[Path]:
    """
    –ü–æ–∏—Å–∫ –≤—Å–µ—Ö .txt –∏ .md —Ñ–∞–π–ª–æ–≤
    """
    docs_dir = Path(docs_path)
    
    if not docs_dir.exists():
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ {docs_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    # –ù–∞–π—Ç–∏ –≤—Å–µ .txt –∏ .md —Ñ–∞–π–ª—ã
    files = list(docs_dir.glob("*.txt")) + list(docs_dir.glob("*.md"))
    
    # –ò—Å–∫–ª—é—á–∏—Ç—å README —Ñ–∞–π–ª—ã
    files = [f for f in files if f.name.lower() != "readme.md"]
    
    if not files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ .txt –∏–ª–∏ .md —Ñ–∞–π–ª–æ–≤ –≤ {docs_path}")
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(files)}")
    for f in files:
        print(f"   - {f.name}")
    
    return files


def load_documents(
    es: Elasticsearch,
    files: List[Path],
    index_name: str,
    chunk_size: int = 500,
    overlap: int = 50
) -> None:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ chunks
    """
    total_docs = 0
    total_chunks = 0
    total_chars = 0
    
    for file_path in files:
        print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path.name}")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å (–Ω–µ–≤–µ—Ä–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        char_count = len(content)
        total_chars += char_count
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks
        chunks = split_into_chunks(content, chunk_size=chunk_size, overlap=overlap)
        chunk_count = len(chunks)
        
        print(f"   üìä –°–∏–º–≤–æ–ª–æ–≤: {char_count:,}")
        print(f"   üî™ Chunks: {chunk_count}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π chunk
        for i, chunk_text in enumerate(chunks, 1):
            doc = {
                "content": chunk_text,
                "filename": file_path.name,
                "chunk_id": i,
                "total_chunks": chunk_count
            }
            
            es.index(index=index_name, document=doc)
        
        total_docs += 1
        total_chunks += chunk_count
        
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {chunk_count} chunks")
    
    print(f"\n" + "="*60)
    print(f"üéâ –ò–¢–û–ì–û:")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_docs}")
    print(f"   Chunks: {total_chunks}")
    print(f"   –°–∏–º–≤–æ–ª–æ–≤: {total_chars:,}")
    print("="*60)


def verify_index(es: Elasticsearch, index_name: str) -> None:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
    """
    # Refresh –∏–Ω–¥–µ–∫—Å–∞
    es.indices.refresh(index=index_name)
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    count = es.count(index=index_name)['count']
    
    # –†–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞
    stats = es.indices.stats(index=index_name)
    size_bytes = stats['indices'][index_name]['total']['store']['size_in_bytes']
    size_mb = size_bytes / (1024 * 1024)
    
    print(f"\nüìä –ü–†–û–í–ï–†–ö–ê –ò–ù–î–ï–ö–°–ê '{index_name}':")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ (chunks): {count}")
    print(f"   –†–∞–∑–º–µ—Ä: {size_mb:.2f} MB")
    
    # –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    result = es.search(index=index_name, body={"size": 1, "query": {"match_all": {}}})
    if result['hits']['hits']:
        doc = result['hits']['hits'][0]['_source']
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä chunk:")
        print(f"   –§–∞–π–ª: {doc['filename']}")
        print(f"   Chunk: {doc['chunk_id']}/{doc['total_chunks']}")
        print(f"   –î–ª–∏–Ω–∞: {len(doc['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–µ–∫—Å—Ç: {doc['content'][:100]}...")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("="*60)
    print("üöÄ –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í –í ELASTICSEARCH")
    print("="*60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    ES_HOST = "localhost"
    ES_PORT = 9200
    INDEX_NAME = "psb_docs"
    DOCS_PATH = "data/documents"
    CHUNK_SIZE = 500  # –†–∞–∑–º–µ—Ä chunk –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    OVERLAP = 50      # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É chunks
    
    try:
        # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        es = check_elasticsearch_connection(ES_HOST, ES_PORT)
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        create_index(es, INDEX_NAME)
        
        # 3. –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        files = find_documents(DOCS_PATH)
        
        # 4. –ó–∞–≥—Ä—É–∑–∫–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ chunks
        load_documents(
            es=es,
            files=files,
            index_name=INDEX_NAME,
            chunk_size=CHUNK_SIZE,
            overlap=OVERLAP
        )
        
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞
        verify_index(es, INDEX_NAME)
        
        print(f"\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤ Kibana: http://localhost:5601")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks: curl http://localhost:9200/{INDEX_NAME}/_count")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())